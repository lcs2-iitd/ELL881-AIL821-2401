---
type: lecture

date: 2024-09-02T11:00:00+4:30

format_date: September 2, 2024 (Monday)

title: "12.1. Pre-training of Causal LMs and In-context Learning"

tldr: "Looking into the procedure for pre-training of causal/auto-regressive language models. Discussion on the in-context learning ability of LLMs."

hide_from_announcments: true

links: 
    - url: /static_files/presentations/121.pdf
      name: slides
    - url: /static_files/presentations/121_scribe.pdf
      name: scribe
    - url: https://youtu.be/xGL15KimKL4
      name: video


thumbnail: /static_files/presentations/121.jpg
---
<!-- Other additional contents using markdown -->
**Suggested Readings:**
- [Improving Language Understanding by Generative Pre-Training](https://hayate-lab.com/wp-content/uploads/2023/05/43372bfa750340059ad87ac8e538c53b.pdf)
- [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [Language Models are Few-Shot Learners](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)
- [Learning To Retrieve Prompts for In-Context Learning](https://aclanthology.org/2022.naacl-main.191.pdf)
- [Transformers Implement Functional Gradient Descent to Learn Non-Linear Functions In Context](https://openreview.net/pdf?id=ah1BlQcLv4)
- [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://aclanthology.org/2022.emnlp-main.759.pdf)


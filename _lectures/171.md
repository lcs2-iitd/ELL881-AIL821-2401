---
type: lecture

date: 2024-10-7T11:00:00+4:30

format_date: October 7, 2024 (Monday)

title: "17.1. Multimodal Models-I"

tldr: "Understanding the architecture and pre-training strategies of multimodal models &ndash; the focus of this lecture is on multimodal understanding involving two modalities (image and text)."

hide_from_announcments: true

links: 
    - url: /static_files/presentations/multimodal_1.pdf
      name: slides
    - url: https://youtu.be/IXp3P1MPB0Q
      name: video

thumbnail: /static_files/presentations/multimodal_1.jpg
---
<!-- Other additional contents using markdown -->
**Suggested Readings:**
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/pdf/2010.11929)
- [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557)
- [ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks](https://arxiv.org/pdf/1908.02265)
- [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020)
- [LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding](https://arxiv.org/pdf/2012.14740)
- [VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding](https://arxiv.org/pdf/2109.14084v2)
- [IMAGEBIND: One Embedding Space To Bind Them All](https://openaccess.thecvf.com/content/CVPR2023/papers/Girdhar_ImageBind_One_Embedding_Space_To_Bind_Them_All_CVPR_2023_paper.pdf)


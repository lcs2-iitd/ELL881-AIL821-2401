---
type: lecture

date: 2024-08-29T11:00:00+5:30

format_date: August 28, 2024 (Wednesday)

title: "10.1. Mixture of Experts-I"

tldr: "Discussion on the Mixture of Experts (MoE) architectural paradigm &ndash; Sparse MoE, Routing Mechanisms and Learning Dynamics."

hide_from_announcments: true

links: 
    - url: /static_files/presentations/101.pdf
      name: slides
    - url: https://youtu.be/wbG71yjomgY
      name: video


thumbnail: /static_files/presentations/101.jpg
---
<!-- Other additional contents using markdown -->
**Suggested Readings:**
- [Adaptive Mixtures of Local Experts](http://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)
- [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://www.cs.toronto.edu/~hinton/absps/Outrageously.pdf)
- [Towards Understanding the Mixture-of-Experts Layer in Deep Learning](https://proceedings.neurips.cc/paper_files/paper/2022/file/91edff07232fb1b55a505a9e9f6c0ff3-Paper-Conference.pdf)

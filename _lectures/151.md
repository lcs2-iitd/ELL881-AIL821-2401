---
type: lecture

date: 2024-09-25T11:00:00+4:30

format_date: September 25, 2024 (Wednesday)

title: "15.1. Efficient LLM Decoding-I"

tldr: "Discussion on various efficient inference/decoding techniques &ndash; KV caching, paged attention and vLLM."

hide_from_announcments: false

links: 
    - url: /static_files/presentations/151.pdf
      name: slides
    - url: /static_files/presentations/151_scribe.pdf
      name: scribe


thumbnail: /static_files/presentations/151.png
---
<!-- Other additional contents using markdown -->
**Suggested Readings:**
- [LLM Inference Serving: Survey of Recent Advances and Opportunities](https://arxiv.org/pdf/2407.12391)
- [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/pdf/2309.06180)

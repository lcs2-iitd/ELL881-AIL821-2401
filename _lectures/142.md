---
type: lecture

date: 2024-09-23T11:00:00+4:30

format_date: September 23, 2024 (Monday)

title: "14.2. Quantization, Pruning & Distillation"

tldr: "Discussion on various model compression techniques &ndash; post-training quantization, QLoRA, magnitude and structured pruning, knowledge distillation."

hide_from_announcments: false

links: 
    - url: /static_files/presentations/142.pdf
      name: slides
    - url: https://youtu.be/FKhjxjjupLA
      name: video


thumbnail: /static_files/presentations/142.jpg
---
<!-- Other additional contents using markdown -->
**Suggested Readings:**
- [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/pdf/2208.07339)
- [QLORA: Efficient Finetuning of Quantized LLMs](https://openreview.net/pdf?id=OUIFPHEgJU)
- [Structured Pruning Learns Compact and Accurate Models](https://arxiv.org/pdf/2204.00408)
- [A Simple and Effective Pruning Approach for Large Language Models](https://arxiv.org/pdf/2306.11695)
- [Distilling the Knowledge in a Neural Network](https://arxiv.org/pdf/1503.02531)
- [Sequence-Level Knowledge Distillation](https://arxiv.org/pdf/1606.07947)

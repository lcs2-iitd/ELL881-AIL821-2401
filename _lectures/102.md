---
type: lecture

date: 2024-08-29T12:00:00+5:30

format_date: August 29, 2024 (Thursday)

title: "10.2. Mixture of Experts-II"

tldr: "Understanding the architecture of Switch Transformers, Mixtral. Discussion on Model Parallelism."

hide_from_announcments: false

links: 
    - url: /static_files/presentations/102.pdf
      name: slides
    - url: /static_files/presentations/102_scribe.pdf
      name: scribe


thumbnail: /static_files/presentations/102.jpg
---
<!-- Other additional contents using markdown -->
**Suggested Readings:**
- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961)
- [Mixtral of Experts](https://arxiv.org/pdf/2401.04088)
- [Paradigms of Parallelism](https://colossalai.org/docs/concepts/paradigms_of_parallelism/)
- [No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization](https://arxiv.org/pdf/2402.18096)

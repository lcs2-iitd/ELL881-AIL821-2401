---
type: lecture

date: 2024-09-05T12:00:00+4:30

format_date: September 5, 2024 (Thursday)

title: "13.1. Alignment of Language Models: Reward Maximization-I"

tldr: "Looking into the reward model for alignment &ndash; modeling the alignment procedure as reinforcement learning, the architecture of reward model, training the reward model, gathering preference data (RLHF vs RLAIF), reward maximization objective."

hide_from_announcments: false

links: 
    - url: /static_files/presentations/131.pdf
      name: slide
    - url: /static_files/presentations/131_scribe.pdf
      name: scribe
    - url: https://youtu.be/f8ZhsaiCr1o
      name: video
    


thumbnail: /static_files/presentations/131.jpg
---
<!-- Other additional contents using markdown -->

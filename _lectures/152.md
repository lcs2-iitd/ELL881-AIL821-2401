---
type: lecture

date: 2024-09-26T12:00:00+4:30

format_date: September 26, 2024 (Thursday)

title: "15.2. Efficient LLM Decoding-II"

tldr: "Discussion on various efficient decoding techniques &ndash; flash decoding, speculative decoding, Medusa and tree attention, prompt-lookup decoding, lookahead decoding."

hide_from_announcments: false

links: 
    - url: /static_files/presentations/151.pdf
      name: slides
    - url: /static_files/presentations/152_scribe.pdf
      name: scribe


thumbnail: /static_files/presentations/151.png
---
<!-- Other additional contents using markdown -->
**Suggested Readings:**
- [Flash-Decoding for long-context inference](https://pytorch.org/blog/flash-decoding/)
- [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/pdf/2211.17192)
- [Accelerating Large Language Model Decoding with Speculative Sampling](https://arxiv.org/pdf/2302.01318)
- [MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads](https://arxiv.org/pdf/2401.10774)
- [Break the Sequential Dependency of LLM Inference Using LOOKAHEAD DECODING](https://arxiv.org/pdf/2402.02057)

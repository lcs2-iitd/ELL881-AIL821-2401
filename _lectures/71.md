---
type: lecture

date: 2024-08-08T12:00:00+4:30

format_date: August 8, 2024 (Thursday)

title: "7.1. Pre-training Strategies"

tldr: "Discussion on ELMo. Understanding the pre-training strategies of encoder-only transformer (BERT) &ndash; Masked Language Modeling."

hide_from_announcments: false

links: 
    - url: /static_files/presentations/71.pdf
      name: slides
   
      
thumbnail: /static_files/presentations/71.jpg
---

<!-- Other additional contents using markdown -->
**Suggested Readings:**
- [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)

---
type: lecture

date: 2024-09-07T10:00:00+4:30

format_date: September 7, 2024 (Saturday)

title: "13.2. Alignment of Language Models: Reward Maximization-II"

tldr: "Looking into different algorithms for training the policy model, which is the LLM, to maximize the reward &ndash; REINFORCE, PPO."

hide_from_announcments: false

links: 
    - url: /static_files/presentations/132.pdf
      name: slides
    - url: /static_files/presentations/132_scribe.pdf
      name: scribe


thumbnail: /static_files/presentations/132.jpg
---
<!-- Other additional contents using markdown -->
**Suggested Readings:**
- [Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347)
- [Training language models to follow instructions with human feedback](https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf)
- [Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs](https://arxiv.org/pdf/2402.14740)
